{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Introduction"
      ],
      "metadata": {
        "id": "97Rkr7QPWCmt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78HE8FLsKN9Q"
      },
      "source": [
        "In this Google Colab notebook, we will walk through how to run BERT-based linguistic feature detectors, applied to data from CORAAL. This is a modified version of our [2023 \"Bridging methods to study sociolectal variation\" workshop tutorial](https://brenocon.com/tut2023). Please don't hesitate to reach out if you have any questions on the material covered in this notebook!\n",
        "\n",
        "We will use a model trained using the method described in [Masis et al.](https://aclanthology.org/2022.fieldmatters-1.2/) (2022). This model will detect instances of 17 African American English morphosyntactic features (features described below, in Section 2). The training data used for this model can be found in `data/CGEdit-ManualGen/AAE.tsv` in [this repository](https://github.com/slanglab/CGEdit), which is associated with the paper linked above. You can also find code for training your own feature detectors in this repository.\n",
        "\n",
        "Contact: Tessa Masis (tmasis@cs.umass.edu), Brendan O'Connor (brenocon@cs.umass.edu)\n",
        "\n",
        "## Use cases for sociolinguistic research\n",
        "\n",
        "We note that these models (i.e. fine-tuned BERT classifiers) do not have perfect precision or recall. However, they often work better than other computational methods (e.g. keyword search, regex) and they take much less time/resources than manual annotation. As with any tool, we recommend you trying it out on the data you're interested in and seeing if the performance makes sense for your research goals.\n",
        "\n",
        "We recommend two general strategies for incorporating these models into your research:\n",
        "\n",
        "1. Augmenting manual annotation\n",
        "  - Run the model on your data; then manually inspect the examples classified as positive, discarding incorrect ones. This strategy still involves manual annotation, but it will only be required for a subset of the original dataset and will guarantee high-precision results.\n",
        "\n",
        "1. Replacing manual annotation\n",
        "  - This strategy is useful in the case that you have a dataset large enough that manual annotation is out of the question (e.g. millions of social media posts). Although there may be errors in the automatic annotation, we assume a low enough error level such that trends or comparisons of interest will still be apparent.\n",
        "\n",
        "Note: The model used in this notebook was trained with the intention of being used on transcript data (i.e. text data that includes nonstandard morphosyntactic features but uses standard orthography). That's why its training data does not include examples with variable spellings. You can certainly still use the model on non-transcript data (e.g. social media data), but performance may be worse since it will be out-of-domain data that the model wasn't necessarily trained for. A straightforward way to address this is to add data from the domain in which you're interested to the training set and then to retrain new models -- this is the strategy we used for a project analyzing AAE in Twitter data (you can find an abstract for that [here](https://scholarworks.umass.edu/scil/vol6/iss1/41/))."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting started\n",
        "To use this notebook, please first create your own copy (without doing this, you will only be able to run the notebook and won't be able to make any edits). To do this, go to the File menu and select 'Save a copy in Drive'. In the copy, click the 'Connect' button in the upper righthand corner. You can now run and edit the notebook!\n",
        "\n",
        "*Software versions: we successfully ran this notebook in July 2023 on Google Colab, which at the time used Python 3.10 and `transformers` version 4.30 (from `pip install` in section \"2. Load and run feature detector classifiers\").*"
      ],
      "metadata": {
        "id": "T9ZJZY17tniY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load and view CORAAL"
      ],
      "metadata": {
        "id": "nsZxTky013xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[CORAAL](https://oraal.uoregon.edu/coraal) is a large public corpus of African American Language data, containing audio recordings and orthographic transcriptions from more than 220 sociolinguistic interviews (you can explore it online [here](http://lingtools.uoregon.edu/coraal/explorer/)).\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "We're going to use a version of the CORAAL transcripts which have already been preprocessed. This version includes 7 components: DCA, DCB, LES, ATL, PRV, ROC, and VLD. It has been preprocessed such that each file is a txt file containing all utterances from a single speaker, where each line in the txt file is a full sentence (i.e. the line ends with a period, exclamation point, or question mark). Non-linguistic sounds denoted by parantheses or angle brackets (e.g. \"(laughing)\", \"\\<cough\\>\") or notes by the transcriber denoted by backslashes (e.g. \"/unintelligible/, \"/inaudible/\") have been removed, as well as the square brackets denoting overlapping speech (although the overlapping speech itself has not been removed).\n",
        "\n",
        "The code used to execute this preprocessing can be found in `code/preprocessCORAAL.py` in  [this repository](https://github.com/slanglab/CGEdit) (the same repository as the one linked in Section 0).\n",
        "\n",
        "## File naming\n",
        "\n",
        "If the speaker is an **interviewee**, their file is named using the following convention:\n",
        "\n",
        "\\<CORAAL component\\>\\_\\<socioeconomic group number\\>\\_\\<age group number\\>\\_\\<gender\\>\\_\\<speaker number\\>\\_\\<audio file number\\>\\.txt\n",
        "\n",
        "For example, the file:\n",
        "\n",
        "DCA\\_se2\\_ag1\\_m\\_05\\_1.txt\n",
        "\n",
        "is from DCA (the Washington, DC 1968 component of CORAAL). The speaker is in socioeconomic group 2, age group 1, and is male number 5. This is the first text file for this speaker.\n",
        "\n",
        "If the speaker is an **interviewer**, then their file is named using this convention:\n",
        "\n",
        "INT-\\<interviewer speaker code\\>-\\<interviewee file name\\>.txt\n",
        "\n",
        "For example, the file:\n",
        "\n",
        "INT-DCA\\_int\\_07-DCA\\_se2\\_ag1\\_m\\_05\\_1.txt\n",
        "\n",
        "contains the utterances from the interviewer (here, DCA\\_int\\_07) corresponding to the DCA\\_se2\\_ag1\\_m\\_05\\_1 transcript file.\n",
        "\n",
        "## More about CORAAL\n",
        "\n",
        "For more details on CORAAL, including transcription practices and information about metadata, please see the [CORAAL User Guide](http://lingtools.uoregon.edu/coraal/userguide/CORAALUserGuide_current.pdf)."
      ],
      "metadata": {
        "id": "XvjR4khJ2Enl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading CORAAL\n",
        "\n",
        "First we download our preprocessed version of CORAAL."
      ],
      "metadata": {
        "id": "uIkbhL7ceHrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tmasis/tutorial2023.git"
      ],
      "metadata": {
        "id": "NOOgUqSTWMkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print all the files by name.\n",
        "\n",
        "<-- You can also see the files by clicking the folder button on the lefthand side (says \"Files\" when you hover). The files will be in `tutorial2023/CORAAL/`.\n"
      ],
      "metadata": {
        "id": "b9iBTVi-eOz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls tutorial2023/CORAAL/"
      ],
      "metadata": {
        "id": "xeGyHDs2GXWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the first few lines from the file ATL_se0_ag1_f_01_1.txt"
      ],
      "metadata": {
        "id": "D8cXaQsheo2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = \"tutorial2023/CORAAL/\"\n",
        "filename = \"ATL_se0_ag2_f_01_1.txt\"\n",
        "\n",
        "with open(test_dir + filename) as f:\n",
        "  for line in list(f)[:10]:\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "o0a6X5Au_8W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at the corresponding interviewer's speech."
      ],
      "metadata": {
        "id": "eNkcIWPNe4uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"INT-ATL_int_01-ATL_se0_ag2_f_01_1.txt\"\n",
        "\n",
        "with open(test_dir + filename) as f:\n",
        "  for line in list(f)[:10]:\n",
        "    print(line)"
      ],
      "metadata": {
        "id": "0uJpO2SXY-WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load and run feature detector classifiers"
      ],
      "metadata": {
        "id": "Oh6y1V_Daja-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now going to load and run the model described above in Section 0. This model will detect instances of 17 African American English morphosyntactic features:\n",
        "1. zero possessive\n",
        "1. zero copula\n",
        "1. double tense\n",
        "1. habitual be\n",
        "1. resultant done\n",
        "1. finna\n",
        "1. come\n",
        "1. double modal\n",
        "1. multiple negation\n",
        "1. negative auxiliary inversion\n",
        "1. non-inverted negative concord\n",
        "1. ain't\n",
        "1. zero 3rd person singular present -s\n",
        "1. is/was generalization\n",
        "1. zero plural -s\n",
        "1. double object\n",
        "1. wh-question\n",
        "\n",
        "For examples of each of these features, please see Table 3 (pg. 19) in [Masis et al.](https://aclanthology.org/2022.fieldmatters-1.2/) (2022).\n"
      ],
      "metadata": {
        "id": "LP_DeU8Gam5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model (in other words, download and get it ready for use)"
      ],
      "metadata": {
        "id": "_8zUg1CUcyr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first do some imports and define some variables\n",
        "!pip install transformers[torch]\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import dataclasses\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "from typing import List, Union, Dict\n",
        "import sys\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Here, we specify the BERT variant we'll be using; for Masis et al. (2022), we tried fine-tuning\n",
        "#   a few different BERTs and this one worked best for detecting AAE features\n",
        "model_name = 'bert-base-cased'\n",
        "# Here we make the directory where we'll save results\n",
        "!mkdir results/\n",
        "# Here we define the directory to save the results\n",
        "out_dir = 'Masis22_CORAAL.tsv'"
      ],
      "metadata": {
        "id": "DEfdppdLfh_2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll now define some classes and a dataloading function\n",
        "\n",
        "class MultitaskModel(transformers.PreTrainedModel):\n",
        "    def __init__(self, encoder, taskmodels_dict):\n",
        "        super().__init__(transformers.PretrainedConfig())\n",
        "        self.encoder = encoder\n",
        "        self.taskmodels_dict = nn.ModuleDict(taskmodels_dict)\n",
        "\n",
        "    @classmethod\n",
        "    def create(cls, model_name, head_type_list):\n",
        "        \"\"\"\n",
        "        Creates each single-feature model (where task == feature), and\n",
        "        has them share the same encoder transformer.\n",
        "        \"\"\"\n",
        "        taskmodels_dict = {}\n",
        "        shared_encoder = transformers.AutoModel.from_pretrained(\n",
        "                model_name,\n",
        "                config=transformers.AutoConfig.from_pretrained(model_name) )\n",
        "\n",
        "        for task_name in head_type_list:\n",
        "            head = torch.nn.Linear(768, 2)\n",
        "            taskmodels_dict[task_name] = head\n",
        "\n",
        "        return cls(encoder=shared_encoder, taskmodels_dict=taskmodels_dict)\n",
        "\n",
        "    def forward(self, inputs, **kwargs):\n",
        "        x = self.encoder(inputs)                # pass thru encoder once\n",
        "        x = x.last_hidden_state[:,0,:]          # get CLS\n",
        "        out_list = []\n",
        "        for task_name,head in self.taskmodels_dict.items(): # pass thru each head\n",
        "            out_list.append(self.taskmodels_dict[task_name](x))\n",
        "        return torch.vstack(out_list)\n",
        "\n",
        "\n",
        "def eval_dataloader(eval_dataset):\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "\n",
        "    data_loader = DataLoader(eval_dataset,\n",
        "                batch_size=64,\n",
        "                sampler=eval_sampler\n",
        "                )\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "class CustomEvalDataset(Dataset):\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"input_ids\": self.text[idx]}"
      ],
      "metadata": {
        "id": "Cg2FwA8kbkSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we define the prediction function\n",
        "def testM(tokenizer, model, test_f):\n",
        "    features_dict = {\"input_ids\": []}\n",
        "\n",
        "    with open(test_f) as r:\n",
        "        for line in r:\n",
        "            if len(line.split()) < 2: continue    # skips utterances that are only one word\n",
        "            tokenized = tokenizer.encode(line.strip(), max_length=64, pad_to_max_length=True, truncation=True)\n",
        "            features_dict[\"input_ids\"].append(torch.LongTensor(tokenized))\n",
        "\n",
        "    features_dict[\"input_ids\"] = torch.stack(features_dict[\"input_ids\"])\n",
        "    features_dict = CustomEvalDataset(features_dict[\"input_ids\"])\n",
        "\n",
        "    # For each head/feature, predict on all sentences if feature is present\n",
        "    dataloader = eval_dataloader(features_dict)\n",
        "    with open(\"results/\"+out_dir,'a') as f:\n",
        "        for steps, inputs in enumerate(dataloader):\n",
        "            for ex in inputs[\"input_ids\"]:\n",
        "                with torch.no_grad():\n",
        "                    output = model(ex.unsqueeze(0).to(device))\n",
        "                output = torch.nn.functional.softmax(output, dim=1)\n",
        "                output = [str(float(x[1])) for x in output]\n",
        "                sent = tokenizer.decode(ex).split()\n",
        "                sent = [e for e in sent if e != '[PAD]' and e != '[CLS]' and e != '[SEP]']\n",
        "                f.write(str(test_f)[11:-6]+\"\\t\"+\" \".join(sent)+\"\\t\"+\"\\t\".join(output)+\"\\n\")"
      ],
      "metadata": {
        "id": "ojxQ0jTzdlIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some final setup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Here we define a head for each feature\n",
        "# The names correspond to these 17 features:     zero possessive, zero copula, double tense, habitual be, resultant done, finna, come,\n",
        "#                             double modal, multiple negation, negative auxiliary inversion, non-inverted negative concord/multiple negation,\n",
        "#                             ain't, zero 3rd person singular present -s, is/was generalization, zero plural -s, double object, wh-question\n",
        "# There are numbers in front of each head name just because that's what I named them when I trained this model\n",
        "head_type_list=[\n",
        "        \"1-zero-poss\",\n",
        "        \"3-zero-copula\",\n",
        "        \"4-double-tense\",\n",
        "        \"5-be-construction\",\"5-resultant-done\",\n",
        "        \"6-finna\",\"6-come\",\"6-double-modal\",\n",
        "        \"7-multiple-neg\",\"7-neg-inversion\",\"7-n-inv-neg-concord\",\"7-aint\",\n",
        "        \"8-zero-3sg-pres-s\",\"8-is-was-gen\",\n",
        "        \"9-zero-pl-s\",\n",
        "        \"10-double-object\",\n",
        "        \"11-wh-qu\"\n",
        "        ]\n",
        "multitask_model = MultitaskModel.create(\n",
        "        model_name=model_name,\n",
        "        head_type_list=head_type_list )\n",
        "\n",
        "multitask_model.to(device)"
      ],
      "metadata": {
        "id": "_JmbVk2Og_Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's copy the model to the VM (~5 min)\n",
        "!wget \"http://hobbes.cs.umass.edu/~tmasis/final.pt\""
      ],
      "metadata": {
        "id": "VE_4JsyQbX0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "checkpoint = torch.load(\"final.pt\", map_location=device)\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "multitask_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "multitask_model.eval()\n",
        "multitask_model.to(device)"
      ],
      "metadata": {
        "id": "c24BddJVdiHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run feature detector classifiers on example sentences"
      ],
      "metadata": {
        "id": "wVHDROYddsYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the feature detectors on CORAAL data, let's first try the detectors on sentences we create."
      ],
      "metadata": {
        "id": "6dflZhlqd6OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we define a mini prediction function to test example sentences\n",
        "def testExample(tokenizer, model, test_sent):\n",
        "    features_dict = {\"input_ids\": []}\n",
        "\n",
        "    tokenized = tokenizer.encode(test_sent.strip(), max_length=64, pad_to_max_length=True, truncation=True)\n",
        "    features_dict[\"input_ids\"].append(torch.LongTensor(tokenized))\n",
        "\n",
        "    features_dict[\"input_ids\"] = torch.stack(features_dict[\"input_ids\"])\n",
        "    features_dict = CustomEvalDataset(features_dict[\"input_ids\"])\n",
        "\n",
        "    # For each head/feature, predict on all sentences if feature is present\n",
        "    dataloader = eval_dataloader(features_dict)\n",
        "    for steps, inputs in enumerate(dataloader):\n",
        "        for ex in inputs[\"input_ids\"]:\n",
        "            with torch.no_grad():\n",
        "                output = model(ex.unsqueeze(0).to(device))\n",
        "            output = torch.nn.functional.softmax(output, dim=1)\n",
        "            output = [str(float(x[1])) for x in output]\n",
        "            sent = tokenizer.decode(ex).split()\n",
        "            sent = [e for e in sent if e != '[PAD]' and e != '[CLS]' and e != '[SEP]']\n",
        "            header = [\"zero possessive\", \"zero copula\", \"double tense\", \"habitual be\", \"resultant done\", \"finna\",\n",
        "                      \"come\", \"double modal\", \"multiple negation\", \"negative auxiliary inversion\",\n",
        "                      \"non-inverted negative concord\", \"ain't\", \"zero 3rd person singular present -s\", \"is/was generalization\",\n",
        "                      \"zero plural -s\", \"double object\", \"wh-question\"]\n",
        "            print(\"example:\\t\" + \" \".join(sent))\n",
        "            for i in range(len(header)):\n",
        "              print(header[i] + \":\\t\" + output[i])"
      ],
      "metadata": {
        "id": "wKggWvNSIscU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code block below, the model is given the example sentence and predicts, for each feature, whether that sentence contains that feature. We can see in the printed output that, for each feature, there is a corresponding value which is the model's prediction for if the feature is in the utterance -- a number close to 0 means the model predicted that the utterance likely does not have the feature and a number close to 1 means it predicted that the utterance likely does have the feature.\n",
        "\n",
        "The example sentence below (taken from twitter) contains both the zero copula and the finna feature. Accordingly, the model gives both of those features high scores (~0.99) and gives low scores for every other feature (< 0.0001).\n",
        "\n",
        "(Note: In theory, scores in the middle, e.g. around 0.5, should correspond to ambiguous utterances in which it's unclear whether or not the utterance contains the given feature. However in practice, differences in score don't always correspond to this -- the model doesn't know explicit information about linguistic structures, so a difference in score may not have a linguistic reason. For example, an utterance may have a score that's lower than another utterance simply because it has a content word that wasn't in the training data or because one word has a nonstandard spelling.)\n"
      ],
      "metadata": {
        "id": "akb397zteCOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"Your boy really still waiting on that pizza. Dominoes finna close soon.\"\n",
        "testExample(tokenizer, multitask_model, example_sentence)"
      ],
      "metadata": {
        "id": "N3V4cerHOkWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is another example taken from twitter. Here we see a high score for habitual be (0.99) and pretty low scores for every other feature (< 0.001), as expected."
      ],
      "metadata": {
        "id": "CkZa0etcgfRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"These little kids basketball games be gettin intense as fuck lol.\"\n",
        "testExample(tokenizer, multitask_model, example_sentence)"
      ],
      "metadata": {
        "id": "GZ041IyvKwVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity #1\n",
        "Play around with example sentences you create!"
      ],
      "metadata": {
        "id": "VGGjWG4fP9BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"\"       # TODO: Insert your example sentence here\n",
        "testExample(tokenizer, multitask_model, example_sentence)"
      ],
      "metadata": {
        "id": "GUmcATM8QKvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Run feature detector classifiers on CORAAL"
      ],
      "metadata": {
        "id": "m2ohYIvLQPbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run the model on the CORAAL data. The code below will run the model on 50 of the files (~1/3 of the total files) and should take ~5min. We'll also only look at interviewee speech files here, not interviewer ones."
      ],
      "metadata": {
        "id": "7953bTIC8i4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "with os.scandir(test_dir) as d:\n",
        "  for test_file in d:\n",
        "    if 'INT' in test_file.name: continue    # This line skips interviewer speech files\n",
        "    print(\"Processing \" + test_file.name + \"...\")\n",
        "    testM(tokenizer, multitask_model, test_file)\n",
        "    print(\"Finished \" + test_file.name)\n",
        "    i += 1\n",
        "    if i == 50: break      # Comment out this line if you'd like to run the model on all of the files"
      ],
      "metadata": {
        "id": "gyilbSqrbvgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results file is stored in `results/Masis22_CORAAL.tsv` (you should also be able to see it by clicking on the Files folder on the lefthand side).\n",
        "\n",
        "The code below converts the results file into a pandas DataFrame, and then prints the total number of utterances which the model has made predictions on."
      ],
      "metadata": {
        "id": "6wxrAWg_9fcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the results!\n",
        "filename = \"results/Masis22_CORAAL.tsv\"\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(filename, sep=\"\\t\", names=[\"speaker\", \"example\", \"zero possessive\", \"zero copula\", \"double tense\", \"habitual be\",\n",
        "                                  \"resultant done\", \"finna\", \"come\", \"double modal\", \"multiple negation\", \"negative auxiliary inversion\",\n",
        "                                  \"non-inverted negative concord\", \"ain't\", \"zero 3rd person singular present -s\", \"is/was generalization\",\n",
        "                                  \"zero plural -s\", \"double object\", \"wh-question\"] )\n",
        "print(\"We have feature detection predictions for \" + str(len(df.index)) + \" utterances.\")"
      ],
      "metadata": {
        "id": "hZWlMkErbw6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we print the first 5 utterances. For each utterance, there is an 'example' column followed by 17 columns corresponding to our 17 morphosyntactic features. The value in the feature columns corresponds to the model's prediction for if that feature is in the utterance, where a 0 means it's predicted the utterance does not have the feature and 1 means it's predicted the utterance does have the feature"
      ],
      "metadata": {
        "id": "OW8Q7BJ1-b3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "XdMrRn1S4fyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assume that 0.5 is an okay threshold and look at all the utterances that are predicted to have zero copula (aka the model predicted a score above 0.5 for that utterance).\n"
      ],
      "metadata": {
        "id": "7dP4fCm47slO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "df.loc[df[\"zero copula\"] > 0.5, [\"example\"]]"
      ],
      "metadata": {
        "id": "7oi61BOErhkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also try other thresholds and see how that affects the results. We note that there's often not a clear cutoff threshold which cleanly divides true positives from true negatives; it makes sense to choose a higher threshold if you'd like high precision (but would result in lower recall), whereas a lower threshold would give you higher recall (although lower precision)."
      ],
      "metadata": {
        "id": "EYx_3a-y_B0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df[\"zero copula\"] > 0.9, [\"example\"]]"
      ],
      "metadata": {
        "id": "5EJki2iQ1Oqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below also prints the speaker corresponding to each utterance and the predicted score for the feature."
      ],
      "metadata": {
        "id": "Ahpa-OMBBQHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df[\"zero copula\"] > 0.5, [\"speaker\", \"example\", \"zero copula\"]]"
      ],
      "metadata": {
        "id": "kPgiNHXoyVwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we sort the utterances by predicted score."
      ],
      "metadata": {
        "id": "yDbKJ3ckBWe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(\"zero copula\", ascending=False).loc[df[\"zero copula\"] > 0.5, [\"example\", \"zero copula\"]]"
      ],
      "metadata": {
        "id": "o9cE4I_gSgxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity #2\n",
        "\n",
        "Take a look at predicted positive instances for different features, and with different thresholds. Different classifiers may have different levels of accuracy (see the rightmost column of Table 7 (pg. 22) in [Masis et al.](https://aclanthology.org/2022.fieldmatters-1.2/) (2022) for our precision@100 results for each feature). In our project using feature detectors on Twitter data (abstract [here](https://scholarworks.umass.edu/scil/vol6/iss1/41/)), we decided to manually calibrate thresholds for each detector."
      ],
      "metadata": {
        "id": "aMuLs37vTfiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of possible feature names:     \"zero possessive\", \"zero copula\", \"double tense\", \"habitual be\", \"resultant done\", \"finna\",\n",
        "#                                  \"come\", \"double modal\", \"multiple negation\", \"negative auxiliary inversion\",\n",
        "#                                  \"non-inverted negative concord\", \"ain't\", \"zero 3rd person singular present -s\", \"is/was generalization\",\n",
        "#                                  \"zero plural -s\", \"double object\", \"wh-question\"\n",
        "\n",
        "\n",
        "feature = \"\"        # TODO: insert feature name here\n",
        "threshold = 0.5     # TODO: modify threshold; can be any number from 0 to 1\n",
        "\n",
        "df.loc[df[feature] > threshold, [\"speaker\", \"example\", feature]]"
      ],
      "metadata": {
        "id": "qq0agXNLzKp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Visualize feature use in (our subset of) CORAAL data"
      ],
      "metadata": {
        "id": "uNundmxOV5wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below will create a bar chart visualizing how much people from each region tend to use AAE features. For simplicity, we use 0.5 as a threshold for each feature, where a score above 0.5 means the model predicts the utterance has the feature and a score below 0.5 means the model predicts the utterance doesn't have the feature. We then calculate average feature frequency over all features."
      ],
      "metadata": {
        "id": "fX7x1TbhBkpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "regions = defaultdict(lambda: [])\n",
        "\n",
        "df2 = df.round(0)     # This rounds each score to either 0 or 1, if it's below or above 0.5, respectively\n",
        "for i in df2.index:\n",
        "  regions[df2[\"speaker\"][i][:3]].extend(list(df2.iloc[i, 2:]))\n",
        "\n",
        "for k,v in regions.items():\n",
        "  regions[k] = sum(v)/len(v)\n",
        "sorted_regions = dict(sorted(regions.items(), key=lambda x : x[1]))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xvals = sorted_regions.keys()\n",
        "yvals = sorted_regions.values()\n",
        "ax.bar(xvals, yvals)\n",
        "ax.set_ylabel(\"Average feature frequency\")\n",
        "ax.set_xlabel(\"Region\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WmjjQUvf3wtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we look at average frequency of just one feature across the regions -- habitual be."
      ],
      "metadata": {
        "id": "XIUlFpEQCREJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regions = defaultdict(lambda: [])\n",
        "\n",
        "for i in df2.index:\n",
        "  regions[df2[\"speaker\"][i][:3]].extend(list(df2.iloc[i, 5:6]))\n",
        "\n",
        "for k,v in regions.items():\n",
        "  regions[k] = sum(v)/len(v)\n",
        "sorted_regions = dict(sorted(regions.items(), key=lambda x : x[1]))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xvals = sorted_regions.keys()\n",
        "yvals = sorted_regions.values()\n",
        "ax.bar(xvals, yvals)\n",
        "ax.set_ylabel(\"Habitual be frequency\")\n",
        "ax.set_xlabel(\"Region\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "COIrEcXwUveu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here we look at average frequency of resultant done across regions."
      ],
      "metadata": {
        "id": "kkDmhs8sCY-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regions = defaultdict(lambda: [])\n",
        "\n",
        "for i in df2.index:\n",
        "  regions[df2[\"speaker\"][i][:3]].extend(list(df2.iloc[i, 6:7]))\n",
        "\n",
        "for k,v in regions.items():\n",
        "  regions[k] = sum(v)/len(v)\n",
        "sorted_regions = dict(sorted(regions.items(), key=lambda x : x[1]))\n",
        "sorted_regions.pop(\"INT\")\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xvals = sorted_regions.keys()\n",
        "yvals = sorted_regions.values()\n",
        "ax.bar(xvals, yvals)\n",
        "ax.set_ylabel(\"Resultant done frequency\")\n",
        "ax.set_xlabel(\"Region\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iUbFI9Z1VnSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also visualize gender stuff! It's a bit harder to look at socioeconomic or age groups, because they're defined differently for different CORAAL componenets; so to look at those variables would require some manual aligning."
      ],
      "metadata": {
        "id": "4v3aCBRPCb28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genders = defaultdict(lambda: [])\n",
        "\n",
        "for i in df2.index:\n",
        "  genders[df2[\"speaker\"][i][12:13]].extend(list(df2.iloc[i, 2:]))\n",
        "\n",
        "for k,v in genders.items():\n",
        "  genders[k] = sum(v)/len(v)\n",
        "sorted_genders = dict(sorted(genders.items(), key=lambda x : x[1]))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xvals = sorted_genders.keys()\n",
        "yvals = sorted_genders.values()\n",
        "ax.bar(xvals, yvals)\n",
        "ax.set_ylabel(\"Average feature frequency\")\n",
        "ax.set_xlabel(\"Gender\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P8lbazmPGlUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity #3\n",
        "Try looking at features distributions for different social variables!"
      ],
      "metadata": {
        "id": "Q9IZ_iDCaIvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "social_vars = defaultdict(lambda: [])\n",
        "\n",
        "for i in df2.index:\n",
        "  speaker_var = df2[\"speaker\"][i][:3]     # TODO: change the slice to correspond to social variable of your choice.\n",
        "                                      #   For example, \"[:3]\" corresponds to region,\n",
        "                                      #   \"[4:7]\" corresponds to socioeconomic group, \"[8:11]\" to age group,\n",
        "                                      #   and \"[12:13]\" to gender\n",
        "\n",
        "  feats = list(df2.iloc[i, 2:])            # TODO: change the slice to correspond to feature(s) of your choice.\n",
        "                                      # For example, \"[i, 2:]\" corresponds to average of all features,\n",
        "                                      # \"[i, 5:6]\" corrresponds to only habitual be,\n",
        "                                      # and \"[i, 6:7]\" corresponds to only resultant done.\n",
        "                                      # Features can be indexed according to placement in this list:\n",
        "          # [\"speaker\", \"example\", \"zero possessive\", \"zero copula\", \"double tense\", \"habitual be\", \"resultant done\", \"finna\",\n",
        "#            \"come\", \"double modal\", \"multiple negation\", \"negative auxiliary inversion\", \"non-inverted negative concord\",\n",
        "#            \"ain't\", \"zero 3rd person singular present -s\", \"is/was generalization\", \"zero plural -s\", \"double object\", \"wh-question\"]\n",
        "\n",
        "  social_vars[speaker_var].extend(feats)\n",
        "\n",
        "for k,v in social_vars.items():\n",
        "  social_vars[k] = sum(v)/len(v)\n",
        "sorted_vars = dict(sorted(social_vars.items(), key=lambda x : x[1]))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "xvals = sorted_vars.keys()\n",
        "yvals = sorted_vars.values()\n",
        "ax.bar(xvals, yvals)\n",
        "ax.set_ylabel(\"Feature frequency\")             # TODO: change feature label here\n",
        "ax.set_xlabel(\"Social variable\")               # TODO: change social variable label here\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WJsrra93OvsJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}